{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch \n",
    "\n",
    "We follow this [tutorial](https://medium.com/@msouza.os/llm-from-scratch-with-pytorch-9f21808c6319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: need to make the machine able to read text. Since it can only understand numbers, need to _encode text into numbers_. Since humans can only understand text, we need to be able to _decode numbers into text_\n",
    "\n",
    "=> need numeric representation of text \n",
    "\n",
    "1. split text into sequence of words (_tokens_)\n",
    "2. for each word (_token_) in model vocabulrary, assign integer\n",
    "\n",
    "There are several different models for tokenizers. Note that model performance during training will be affected by chosen tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_vocab(dataset:str)->dict[str:str]:\n",
    "        \"\"\"\n",
    "        Create vocabulary\n",
    "\n",
    "        :param dataset: Text data to be used to define vocabulary\n",
    "        :type dataset: str\n",
    "\n",
    "        :returns: (word) vocabulary  \n",
    "        :rtype: dict[str:int]\n",
    "        \"\"\"\n",
    "\n",
    "        # CREATE VOCABULRARY\n",
    "        pattern_ = re.compile(r'(?<=\\w)(?=[^\\w])|(?<=[^\\w])(?=\\w)|(?<=[^\\w])(?=\\s)|(?<=[^\\w])(?=[^w])') # pattern splits string into words, spaces and special characters\n",
    "        words_in_data = pattern_.split(dataset)\n",
    "        vocab = {tkn:idx for idx,tkn in enumerate(sorted(set(words_in_data)))}\n",
    "\n",
    "        # ADD UNKNOWN TOKEN\n",
    "        vocab['<ukn>'] = len(vocab)\n",
    "\n",
    "        return vocab\n",
    "    \n",
    "\n",
    "    def __init__(self,vocab:dict[str,int]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize tokenizer\n",
    "\n",
    "        :param vocab: vocabulary to intitalize\n",
    "        :type vocab: dict[str:int]\n",
    "        \"\"\"\n",
    "        self.vocab_encode = {str(k): int(v) for k, v in vocab.items()} # encode vocab: key:value = char:int\n",
    "        self.vocab_decode = {v: k for k, v in self.vocab_encode.items()} # decode vocab key:value = int:char\n",
    "        self.pattern = re.compile(r'(?<=\\w)(?=[^\\w])|(?<=[^\\w])(?=\\w)|(?<=[^\\w])(?=\\s)|(?<=[^\\w])(?=[^w])')\n",
    "\n",
    "    def encode(self,text:str):\n",
    "        \"\"\"\n",
    "        Encode text in (lvl: character)\n",
    "\n",
    "        :param text: Input text to encode\n",
    "        :type text: str\n",
    "\n",
    "        :returns: encoded text (list with token indices)\n",
    "        :rtype: list[int]\n",
    "        \"\"\"\n",
    "\n",
    "        text_ = self.pattern.split(text)\n",
    "        encoded_txt = [self.vocab_encode.get(word,self.vocab_encode['<ukn>']) for word in text_]\n",
    "        return encoded_txt\n",
    "    \n",
    "    def decode(self,idxs:list[int]):\n",
    "        \"\"\"\n",
    "        Decode a list of token indices\n",
    "\n",
    "        :param idxs: List of token indices\n",
    "        :type idxs: list[int]\n",
    "\n",
    "        :returns: Decoded text\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "\n",
    "        decoded_txt = [self.vocab_decode.get(idx,'<ukn>') for idx in idxs]\n",
    "\n",
    "        return ''.join(decoded_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc = [4, 0, 5, 0, 9, 9]\n",
      "\n",
      "dec = 'Hallo Welt <ukn><ukn>'\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Hallo Welt! Wie geht es Dir?'\n",
    "vocab = Tokenizer.create_vocab(dataset)\n",
    "\n",
    "tokenize = Tokenizer(vocab)\n",
    "\n",
    "test = 'Hallo Welt :)'\n",
    "\n",
    "enc = tokenize.encode(test)\n",
    "dec = tokenize.decode(enc)\n",
    "\n",
    "print(f'{enc = }\\n')\n",
    "print(f'{dec = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** We want the computer to understand the symmantics (not only syntax). Thus we need to teach it what words (characters) are _similar_. To do so, we use vector represetnations (**embeddings**) of the tokens. These embeddings come with learnable parameters that can be adjusted during training to imporve the representation. The embedding is thus a map from the vocabulary $V$ to some vector space $X$: $E \\colon V \\to X$. If $\\dim(V) = N$ (length of vodabulary) and $\\dim(X) = d$, then the embedding is a _linear_ map $E \\colon \\mathbb{R}^N \\to \\mathbb{R}^d$ so a $N \\times d$ matrix. \n",
    "\n",
    "Notice that the larger $d$, the more parameters can be learned and hence the more the model can learn more details about the dataset at the expense of computation time.\n",
    "\n",
    "\n",
    "**Implementation:** \n",
    "\n",
    "1. Assign to each token a random vector of parameters (which will be learned during training). Assume that the length of the vocabulary is $N$, and we embedd into a vector space of dimension $d$, the embedding space will be $\\mathbb{R}^N \\times \\mathbb{R}^d$ which can be represented as a matrix with $N$ rows and $d$ columns.\n",
    "\n",
    "2. To compute similarity, use _cosine_ distance: for $x,y \\in \\mathbb{R}^d$, define their cosine-distance by $d(x,y) = \\cos( \\sphericalangle (x,y) ) \\in [-1,1]$\n",
    "\n",
    "Note that \n",
    "\n",
    "$$ \\cos( \\sphericalangle (x,y) ) = \\frac{\\langle x, y \\rangle}{\\lVert x \\rVert \\lVert y \\rVert}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!** LLMs = Transformer based models\n",
    "\n",
    "**Transformer model architecture:** [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "1. composed of 1 _encoder_ and 1 _decoder_ block (using attention mechanism called _Scaled Dot-Product Attention_)\n",
    "\n",
    "2. How they work\n",
    "    * Encoder block: tries to analyze each word considering the entire text context\n",
    "    * Decoder block: masks all future words at some specific position and uses only the _previous_ words available to analyze the current one\n",
    "    * Main block: _Multi-Head Attention_ (!! key component of LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "**Input:**\n",
    "* Q: query = information we're trying to find (relationship between tokens)\n",
    "* K: Key =  where you will find this information\n",
    "* V: Value\n",
    "\n",
    "\n",
    "\n",
    "**architecture:** \n",
    "\n",
    "```\n",
    "Q --> |\n",
    "      | MatMult --> Scale --> Mask(opt.) --> SoftMax --> |\n",
    "K --> |                                                  |   \n",
    "                                                         | -- MatMult -->\n",
    "                                                         |\n",
    "V ------------------------------------------------------>|\n",
    "```\n",
    "\n",
    "\n",
    "$Q,K,V$ are calculated by linear transformations (simple linear layers) from the embedded text. The workflow is roughly as follows: for $T \\in Mat(\\mathbb{R},N\\times N)$\n",
    "\n",
    "$\\text{text (str)} \\stackrel{E\\;(\\text{embedding})}{\\longrightarrow} Q,K \\in \\mathbb{R}^N \\times \\mathbb{R}^d \\stackrel{T}{\\longrightarrow} Q',K' \\in \\mathbb{R}^N \\times \\mathbb{R}^d  \\stackrel{Q K^t}{\\longrightarrow} R \\in \\mathbb{R}^N \\times \\mathbb{R}^N$ \n",
    "\n",
    "The output of this sequence is a $N\\times N$ matrix $R$ encoding the _relationship_ between our tokens. \n",
    "\n",
    "Next, we need to scale (to avoid 1-hot encoding) by $\\sqrt{N}$ ($N$ is called the _embedding dimension_) and then feed it into a SoftMax function to normalize each value to (0,1) such that each row sums up to 1. This defines the so-called _attention_\n",
    "\n",
    "$$A = \\frac{Q K^t}{\\sqrt{N}}$$\n",
    "\n",
    "Finally, we need to multiply it with the $V$ to obtain a \"vector representation\" if the information we were looking for:\n",
    "\n",
    "$$ \\text{out} = A V$$ \n",
    "\n",
    "_Remark_: It seems to be custom to scale A by the _embedding size_ $d$\n",
    "\n",
    "_NOTE_:  The attention mechanisim for transformers is trying to predict **every single token** in our sequence and not only the last one.\n",
    "However, during training we actually want to predict only the next token given a sequence of tokens as _context_. We could even look at a Markov Chain by considering only the previous token to predict the next one. The problem is that the output $out = A V$ considers _all_ possible tokens: the entire attention matrix $A$ is used.\n",
    "To overcome this problem, we mask $A$. We can multiply $A$ simply by a lower triangular matrix $M$ with all non-zero entries being 1. The mask then hides all consecutive tokens of all samples.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.size() = torch.Size([1, 9, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTATION\n",
    "torch.random.manual_seed(seed=42)\n",
    "\n",
    "\n",
    "# DATA\n",
    "text = \"Hi! I am Donald.\"\n",
    "vocab = Tokenizer.create_vocab(text)\n",
    "tokenize = Tokenizer(vocab)\n",
    "\n",
    "tokens = tokenize.encode(text) # [4, 1, 0, 5, 0, 6, 0, 3, 2]\n",
    "tokens.reverse()\n",
    "\n",
    "# PARAMETERS\n",
    "# vocab_size = max(tokens) + 1 # numbers of tokens (classes) to predict \n",
    "vocab_size = max(vocab.values())  #N = numbers of tokens (classes) to predict (last class is <ukn> but 0 based)\n",
    "emb_dim = 3 # d = size of vecrep of each token\n",
    "context = len(tokens) # context size of model\n",
    "\n",
    "# LAYERS\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim) # tokens = (N,) -> (N,3)\n",
    "query = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False) # linear transformation on 2nd idx (N,3) -> (N,3) @ (3,3) = (N,3)\n",
    "key = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "value = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "\n",
    "# MASK\n",
    "ones = torch.ones(size=[context, context], dtype=torch.float)\n",
    "mask = torch.tril(input=ones) # lower triangular matrix \n",
    "\n",
    "# TOKEN IDXS FOR POS EMBEDDING\n",
    "tkn_idxs = torch.arange(context, dtype=torch.long)\n",
    "\n",
    "# FORWARD PASS\n",
    "t_tokens = torch.tensor(tokens).unsqueeze(dim=0) # (N,) -> (1,N)\n",
    "\n",
    "x = embedding(t_tokens) # (1,N) -> (1,N,d) embedded vectors\n",
    "\n",
    "B, T, C = x.size()\n",
    "\n",
    "# apply linear transformation T\n",
    "Q = query(x) # (1,N,d) -> (1,N,d) @ (1,d,d) = (1,N,d)\n",
    "K = key(x) \n",
    "V = value(x) \n",
    "\n",
    "# matmul\n",
    "QK = Q @ K.transpose(-2, -1) # (1,N,d) @ (1,d,N) -> (1,N,N)\n",
    "A = QK  * C**-0.5 # attention matrix \n",
    "A.masked_fill_(mask[:T,:T] == 0, float(\"-inf\")) # applying mask\n",
    "A = F.softmax(input=A, dim=-1) # apply SoftMax: (1,N,N) normalizing to 0 and 1 in embedding dimension\n",
    "\n",
    "out = A @ V # (1,N,N) @ (1,N,d) -> (1,N,d)\n",
    "\n",
    "# print(f'{out = }\\n') # new data representation\n",
    "print(f'{out.size() = }\\n') \n",
    "\n",
    "# x_before = x.detach().clone()\n",
    "x_after = x.detach().clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x before reversing the order of the token sequence\n",
      "tensor([[[-1.1109,  0.0418, -0.2516],\n",
      "         [-2.1055,  0.6784,  1.0783],\n",
      "         [ 1.9269,  1.4873,  0.9007],\n",
      "         [ 0.8599, -0.3097, -0.3957],\n",
      "         [ 1.9269,  1.4873,  0.9007],\n",
      "         [ 0.8034, -0.6216, -0.5920],\n",
      "         [ 1.9269,  1.4873,  0.9007],\n",
      "         [-0.6866, -0.4934,  0.2415],\n",
      "         [ 0.8008,  1.6806,  0.3559]]])\n"
     ]
    }
   ],
   "source": [
    "print('x before reversing the order of the token sequence')\n",
    "print(x_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after reversing the order of the token sequence\n",
      "tensor([[[ 0.8008,  1.6806,  0.3559],\n",
      "         [-0.6866, -0.4934,  0.2415],\n",
      "         [ 1.9269,  1.4873,  0.9007],\n",
      "         [ 0.8034, -0.6216, -0.5920],\n",
      "         [ 1.9269,  1.4873,  0.9007],\n",
      "         [ 0.8599, -0.3097, -0.3957],\n",
      "         [ 1.9269,  1.4873,  0.9007],\n",
      "         [-2.1055,  0.6784,  1.0783],\n",
      "         [-1.1109,  0.0418, -0.2516]]])\n"
     ]
    }
   ],
   "source": [
    "print('x after reversing the order of the token sequence')\n",
    "print(x_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the entries in x _before_ and _after_ reversing the order of the input sequence of tokens are essentially the same up to orientation. \n",
    "The columns (same for rows) contain the same numbers but in reversed order, as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:          after:          diff:\n",
      "-1.111          -1.111           0.000\n",
      "-2.106          -2.106           0.000\n",
      " 1.927           1.927           0.000\n",
      " 0.860           0.860           0.000\n",
      " 1.927           1.927           0.000\n",
      " 0.803           0.803           0.000\n",
      " 1.927           1.927           0.000\n",
      "-0.687          -0.687           0.000\n",
      " 0.801           0.801           0.000\n"
     ]
    }
   ],
   "source": [
    "x_before_col1 = [elem for elem in x_before[0][:,0]]\n",
    "x_after_col1 = [elem for elem in x_after[0][:,0]]\n",
    "x_after_col1.reverse()\n",
    "\n",
    "print(f'before: {\"after:\":>15} {\"diff:\":>14}')\n",
    "for i in range(len(x_before_col1)):\n",
    "    print(f'{x_before_col1[i]: .3f} {x_after_col1[i]: >15.3f} {x_before_col1[i] - x_after_col1[i]: >15.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location of words in sentences matter.\n",
    "The vector representation should thereofre depend on the order of the token sequence.\n",
    "\n",
    "However, the attention mechanism above considers every token all at once in order to get a sense of _context_. Hence, we need to encode the positions by hand. This can be done by introducing another set of _positional_ parameters which can be learned. We simply define yet another embedding (of dimension context x emb_dim) and add it to our token embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = tensor([[[-3.1104, -0.4777, -2.1211],\n",
      "         [-1.6634, -0.0585, -1.0139],\n",
      "         [-0.8581, -0.0070, -0.5185],\n",
      "         [ 0.0281,  0.2477,  0.1658],\n",
      "         [-0.4765,  0.1084, -0.1927],\n",
      "         [-0.0140,  0.1900,  0.1065],\n",
      "         [ 0.0133,  0.1353,  0.1065],\n",
      "         [ 0.1216,  0.0141,  0.0771],\n",
      "         [ 0.0057,  0.0227,  0.0102]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "out.size() = torch.Size([1, 9, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTATION\n",
    "torch.random.manual_seed(seed=42)\n",
    "\n",
    "\n",
    "# DATA\n",
    "text = \"Hi! I am Donald.\"\n",
    "vocab = Tokenizer.create_vocab(text)\n",
    "tokenize = Tokenizer(vocab)\n",
    "\n",
    "tokens = tokenize.encode(text) # [4, 1, 0, 5, 0, 6, 0, 3, 2]\n",
    "tokens.reverse()\n",
    "\n",
    "# PARAMETERS\n",
    "# vocab_size = max(tokens) + 1 # numbers of tokens (classes) to predict \n",
    "vocab_size = max(vocab.values())  #N = numbers of tokens (classes) to predict (last class is <ukn> but 0 based)\n",
    "emb_dim = 3 # d = size of vecrep of each token\n",
    "context = len(tokens) # context size of model\n",
    "\n",
    "# LAYERS\n",
    "pos_emb = nn.Embedding(num_embeddings=context, embedding_dim=emb_dim) # !! positional encoding \n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim) # tokens = (N,) -> (N,3)\n",
    "query = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False) # linear transformation on 2nd idx (N,3) -> (N,3) @ (3,3) = (N,3)\n",
    "key = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "value = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "\n",
    "# MASK\n",
    "ones = torch.ones(size=[context, context], dtype=torch.float)\n",
    "mask = torch.tril(input=ones) # lower triangular matrix \n",
    "\n",
    "# TOKEN IDXS FOR POS EMBEDDING\n",
    "tkn_idxs = torch.arange(context, dtype=torch.long)\n",
    "\n",
    "# FORWARD PASS\n",
    "t_tokens = torch.tensor(tokens).unsqueeze(dim=0) # (N,) -> (1,N)\n",
    "\n",
    "x = embedding(t_tokens) # (1,N) -> (1,N,d) embedded vectors\n",
    "x += pos_emb(tkn_idxs) # !! add pos embeddings\n",
    "\n",
    "B, T, C = x.size()\n",
    "\n",
    "# apply linear transformation T\n",
    "Q = query(x) # (1,N,d) -> (1,N,d) @ (1,d,d) = (1,N,d)\n",
    "K = key(x) \n",
    "V = value(x) \n",
    "\n",
    "# matmul\n",
    "QK = Q @ K.transpose(-2, -1) # (1,N,d) @ (1,d,N) -> (1,N,N)\n",
    "A = QK  * C**-0.5 # attention matrix \n",
    "A.masked_fill_(mask[:T,:T] == 0, float(\"-inf\")) # applying mask\n",
    "A = F.softmax(input=A, dim=-1) # apply SoftMax: (1,N,N) normalizing to 0 and 1 in embedding dimension\n",
    "\n",
    "out = A @ V # (1,N,N) @ (1,N,d) -> (1,N,d)\n",
    "\n",
    "print(f'{out = }\\n') # new data representation\n",
    "print(f'{out.size() = }\\n') \n",
    "\n",
    "# x_before = x.detach().clone()\n",
    "x_after = x.detach().clone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, comparing x before and after, we see that we get truely different representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:          after:          diff:\n",
      " 3.205          -0.280           3.484\n",
      "-3.195          -1.152          -2.042\n",
      "-0.889          -1.069           0.180\n",
      " 1.705          -0.160           1.866\n",
      "-1.956          -1.956           0.000\n",
      "-0.859           1.007          -1.866\n",
      "-1.069          -0.889          -0.180\n",
      " 0.284          -1.759           2.042\n",
      "-0.701           2.783          -3.484\n"
     ]
    }
   ],
   "source": [
    "x_before_col1 = [elem for elem in x_before[0][:,0]]\n",
    "x_after_col1 = [elem for elem in x_after[0][:,0]]\n",
    "x_after_col1.reverse()\n",
    "\n",
    "print(f'before: {\"after:\":>15} {\"diff:\":>14}')\n",
    "for i in range(len(x_before_col1)):\n",
    "    print(f'{x_before_col1[i]: .3f} {x_after_col1[i]: >15.3f} {x_before_col1[i] - x_after_col1[i]: >15.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', '!', ' ', 'My', ' ', 'name', ' ', 'is', ' ', 'Donald', '.', ' ', 'What', ' ', 'is', ' ', 'your', ' ', 'name', '?', ' ', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "spltting_pattern = re.compile(r'(?<=\\w)(?=[^\\w])|(?<=[^\\w])(?=\\w)|(?<=[^\\w])(?=\\s)|(?<=[^\\w])(?=[^w])')\n",
    "foo = 'Hi!! My name is Donald. What is your name? :)'\n",
    "res = spltting_pattern.split(foo)\n",
    "print(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
